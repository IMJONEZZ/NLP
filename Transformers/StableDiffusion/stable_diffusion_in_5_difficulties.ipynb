{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level 1\n",
    "# !pip install --upgrade diffusers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade tokenizers\n",
    "# !pip install --upgrade datasets\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_auth_token=access_token).to(\"cuda\") #use revision='fp16' and torch_dtype=torch.float16 for low memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of a horse riding an astronaut on Mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"./images/horse_rides_astronaut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 2\n",
    "from torch import autocast\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(images, **kwargs):\n",
    "    return images, False\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.safety_checker = dummy\n",
    "n_images = 3\n",
    "prompts = [\n",
    "    \"masterpiece, best quality, a photo of a horse riding an astronaut, trending on artstation, photorealistic, qhd, rtx on, 8k\"\n",
    "] * n_images\n",
    "with autocast(\"cuda\"):\n",
    "    images = pipe(prompts, num_inference_steps=28).images\n",
    "image_grid(images, rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level 3\n",
    "from diffusers import UNet2DConditionModel, StableDiffusionPipeline, AutoencoderKL, LMSDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import autocast\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder='vae', use_auth_token=access_token)\n",
    "# vae.save_pretrained('./models/vae')\n",
    "vae = AutoencoderKL.from_pretrained('./models/vae/').to(\"cuda\")\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# tokenizer.save_pretrained('./tokenizers/')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('./tokenizers/')\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n",
    "# text_encoder.save_pretrained('./models/text_encoder')\n",
    "text_encoder = CLIPTextModel.from_pretrained('./models/text_encoder/').to(\"cuda\")\n",
    "\n",
    "# model = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder='unet', use_auth_token=access_token).to(\"cuda\")\n",
    "# model.save_pretrained('./models/sd_v1-5')\n",
    "model = UNet2DConditionModel.from_pretrained('./models/sd_v1-5/').to(\"cuda\")\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear', num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeds(prompt):\n",
    "  # Tokenize text and get embeddings\n",
    "  text_input = tokenizer(\n",
    "      prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "      truncation=True, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n",
    "\n",
    "  # Do the same for unconditional embeddings\n",
    "  uncond_input = tokenizer(\n",
    "      [''] * len(prompt), padding='max_length',\n",
    "      max_length=tokenizer.model_max_length, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n",
    "\n",
    "  # Cat for final embeddings\n",
    "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "  return text_embeddings\n",
    "\n",
    "test_embeds = get_text_embeds(['an amazingly cool anime character'])\n",
    "print(test_embeds)\n",
    "print(test_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "                    num_inference_steps=28, guidance_scale=11, latents=None,\n",
    "                    return_all_latents=False):\n",
    "  if latents is None:\n",
    "    latents = torch.randn((text_embeddings.shape[0] // 2, model.in_channels, \\\n",
    "                           height // 8, width // 8))\n",
    "  latents = latents.to(\"cuda\")\n",
    "\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "  latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "  latent_hist = [latents]\n",
    "  with autocast('cuda'):\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "      sigma = scheduler.sigmas[i]\n",
    "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = model(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
    "      latent_hist.append(latents)\n",
    "  \n",
    "  if not return_all_latents:\n",
    "    return latents\n",
    "\n",
    "  all_latents = torch.cat(latent_hist, dim=0)\n",
    "  return all_latents\n",
    "\n",
    "test_latents = produce_latents(test_embeds)\n",
    "print(test_latents)\n",
    "print(test_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img_latents(latents):\n",
    "  latents = 1 / 0.18215 * latents\n",
    "\n",
    "  with torch.no_grad():\n",
    "    imgs = vae.decode(latents)['sample']\n",
    "\n",
    "  imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "  imgs = imgs.detach().cpu().permute(0, 2, 3, 1)\n",
    "  imgs = (imgs + 1.0) * 127.5\n",
    "  imgs = imgs.numpy().astype(np.uint8)\n",
    "  pil_images = [Image.fromarray(image) for image in imgs]\n",
    "  return pil_images\n",
    "\n",
    "imgs = decode_img_latents(test_latents)\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=28, guidance_scale=11, latents=None):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "        \n",
    "    #Prompts -> text embeddings\n",
    "    text_embeds = get_text_embeds(prompts)\n",
    "    \n",
    "    #Text embeddings -> img latents\n",
    "    latents = produce_latents(text_embeds, height=height, width=width, latents=latents, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale)\n",
    "    \n",
    "    #Img latents -> imgs\n",
    "    imgs = decode_img_latents(latents)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = prompt_to_img(['Super cool fantasty knight, intricate armor, 8k']*4, 512, 512, 28, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(imgs, rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 3.5 - similar images and img2img\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
    "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
    "                  batch_size=2):\n",
    "  if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "\n",
    "  # Prompts -> text embeds\n",
    "  text_embeds = get_text_embeds(prompts)\n",
    "\n",
    "  # Text embeds -> img latents\n",
    "  latents = produce_latents(\n",
    "      text_embeds, height=height, width=width, latents=latents,\n",
    "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "      return_all_latents=return_all_latents)\n",
    "  \n",
    "  # Img latents -> imgs\n",
    "  all_imgs = []\n",
    "  for i in tqdm(range(0, len(latents), batch_size)):\n",
    "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
    "    all_imgs.extend(imgs)\n",
    "\n",
    "  return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Steampunk airship bursting through the clouds, cyberpunk art'\n",
    "latents = torch.randn((1, model.in_channels, 512 // 8, 512 // 8))\n",
    "img = prompt_to_img(prompt, num_inference_steps=20, latents=latents)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_latents(latents, scale=0.1):\n",
    "  noise = torch.randn_like(latents)\n",
    "  new_latents = (1 - scale) * latents + scale * noise\n",
    "  return (new_latents - new_latents.mean()) / new_latents.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_latents = perturb_latents(latents, 0.4)\n",
    "img = prompt_to_img(prompt, num_inference_steps=20, latents=new_latents)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Upright squid'\n",
    "img = prompt_to_img(prompt, num_inference_steps=30)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img_latents(imgs):\n",
    "  if not isinstance(imgs, list):\n",
    "    imgs = [imgs]\n",
    "\n",
    "  img_arr = np.stack([np.array(img) for img in imgs], axis=0)\n",
    "  img_arr = img_arr / 255.0\n",
    "  img_arr = torch.from_numpy(img_arr).float().permute(0, 3, 1, 2)\n",
    "  img_arr = 2 * (img_arr - 0.5)\n",
    "\n",
    "  latent_dists = vae.encode(img_arr.to(\"cuda\"))\n",
    "  latent_samples = latent_dists.latent_dist.sample()\n",
    "  latent_samples *= 0.18215\n",
    "\n",
    "  return latent_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_latents = encode_img_latents([img])\n",
    "dec_img = decode_img_latents(img_latents)[0]\n",
    "dec_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New scheduler for img-to-img\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012,\n",
    "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "                    num_inference_steps=50, guidance_scale=7.5, latents=None,\n",
    "                    return_all_latents=False, start_step=10):\n",
    "  if latents is None:\n",
    "    latents = torch.randn((text_embeddings.shape[0] // 2, model.in_channels, \\\n",
    "                           height // 8, width // 8))\n",
    "  latents = latents.to(\"cuda\")\n",
    "\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "  if start_step > 0:\n",
    "    start_timestep = scheduler.timesteps[start_step]\n",
    "    start_timesteps = start_timestep.repeat(latents.shape[0]).long()\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    latents = scheduler.add_noise(latents, noise, start_timesteps)\n",
    "\n",
    "  latent_hist = [latents]\n",
    "  with autocast('cuda'):\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps[start_step:])):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = model(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, t, latents)['prev_sample']\n",
    "      latent_hist.append(latents)\n",
    "  \n",
    "  if not return_all_latents:\n",
    "    return latents\n",
    "\n",
    "  all_latents = torch.cat(latent_hist, dim=0)\n",
    "  return all_latents\n",
    "\n",
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
    "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
    "                  batch_size=2, start_step=0):\n",
    "  if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "\n",
    "  # Prompts -> text embeds\n",
    "  text_embeds = get_text_embeds(prompts)\n",
    "\n",
    "  # Text embeds -> img latents\n",
    "  latents = produce_latents(\n",
    "      text_embeds, height=height, width=width, latents=latents,\n",
    "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "      return_all_latents=return_all_latents, start_step=start_step)\n",
    "  \n",
    "  # Img latents -> imgs\n",
    "  all_imgs = []\n",
    "  for i in tqdm(range(0, len(latents), batch_size)):\n",
    "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
    "    all_imgs.extend(imgs)\n",
    "\n",
    "  return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Squidward'\n",
    "img = prompt_to_img(prompt, num_inference_steps=30, latents=img_latents,\n",
    "                    start_step=20)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 4 - AUTOMATIC1111\n",
    "#!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
    "#https://rentry.org/voldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level 5 - Deforum, Vid2Vid, Textual Inversion Dreambooth, Negative Prompts, Fine-Tuning, etc.\n",
    "from accelerate import Accelerator\n",
    "from pathlib import Path\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import hashlib\n",
    "import itertools\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from diffusers.optimization import get_scheduler\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textual Inversion Locally Add \"perfect prompts\"\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=4,\n",
    "    mixed_precision=\"no\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accelerator.is_main_process:\n",
    "    os.make_dirs(\"./textual_inversion_outputs/\", exist_ok=True)\n",
    "    \n",
    "tokenizer = CLIPTokenizer.from_pretrained('./tokenizers/')\n",
    "\n",
    "placeholder = \"<pokemon-sprite>\"\n",
    "initializer = \"sprite\"\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {placeholder}.\"\n",
    "        \"Please pass a different placeholder that isn't already in\"\n",
    "        \"the tokenizer.\"\n",
    "    )\n",
    "\n",
    "token_ids = tokenizer.encode(initializer, add_special_tokens=False)\n",
    "if len(token_ids) > 1:\n",
    "    raise ValueError(\n",
    "        \"The initializer token must be a single token,\"\n",
    "        \"try something shorter.\"\n",
    "    )\n",
    "initializer_token_id = token_ids[0]\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained('./models/text_encoder/')\n",
    "vae = AutoencoderKL.from_pretrained('./models/vae/')\n",
    "model = UNet2DConditionModel.from_pretrained('./models/sd_v1-5/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(model.parameters())\n",
    "\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters()\n",
    ")\n",
    "freeze_params(params_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-04 * 4 * 1 * accelerator.num_processes #lr * gradient accumulation steps * training_batch_size * number of processes\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    text_encoder.get_input_embeddings().parameters(), #Only optimize the embeddings\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-08\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    beta_start=0.00085, \n",
    "    beta_end=0.012, \n",
    "    beta_schedule='scaled_linear', \n",
    "    num_train_timesteps=1000, \n",
    "    set_alpha_to_one=False, \n",
    "    skip_prk_steps=True, \n",
    "    steps_offset=1, \n",
    "    clip_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_style_templates_small = [\n",
    "    \"a painting in the style of {}\",\n",
    "    \"a rendering in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"the painting in the style of {}\",\n",
    "    \"a clean painting in the style of {}\",\n",
    "    \"a dirty painting in the style of {}\",\n",
    "    \"a dark painting in the style of {}\",\n",
    "    \"a picture in the style of {}\",\n",
    "    \"a cool painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a bright painting in the style of {}\",\n",
    "    \"a cropped painting in the style of {}\",\n",
    "    \"a good painting in the style of {}\",\n",
    "    \"a close-up painting in the style of {}\",\n",
    "    \"a rendition in the style of {}\",\n",
    "    \"a nice painting in the style of {}\",\n",
    "    \"a small painting in the style of {}\",\n",
    "    \"a weird painting in the style of {}\",\n",
    "    \"a large painting in the style of {}\",\n",
    "]\n",
    "\n",
    "class TextualInversionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        learnable_property=\"object\",  # [object, style]\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learnable_property = learnable_property\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL.Image.LINEAR,\n",
    "            \"bilinear\": PIL.Image.BILINEAR,\n",
    "            \"bicubic\": PIL.Image.BICUBIC,\n",
    "            \"lanczos\": PIL.Image.LANCZOS,\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        text = random.choice(self.templates).format(placeholder_string)\n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            h, w, = (\n",
    "                img.shape[0],\n",
    "                img.shape[1],\n",
    "            )\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example\n",
    "\n",
    "train_dataset = TextualInversionDataset(\n",
    "        data_root=\"./training/\",\n",
    "        tokenizer=tokenizer,\n",
    "        size=512,\n",
    "        placeholder_token=placeholder,\n",
    "        repeats=100,\n",
    "        learnable_property=\"object\",\n",
    "        center_crop=False,\n",
    "        set=\"train\",\n",
    "    )\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / 4)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=3000 * 4,\n",
    ")\n",
    "\n",
    "text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    text_encoder, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "vae.to(accelerator.device)\n",
    "model.to(accelerator.device)\n",
    "\n",
    "vae.eval()\n",
    "model.eval()\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / 4)\n",
    "num_train_epochs = math.ceil(3000 / num_update_steps_per_epoch)\n",
    "\n",
    "# Train!\n",
    "total_batch_size = 1 * accelerator.num_processes * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(3000), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Steps\")\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    text_encoder.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(text_encoder):\n",
    "            \n",
    "            latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            noise = torch.randn(latents.shape).to(latents.device)\n",
    "            bsz = latents.shape[0]\n",
    "            \n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device\n",
    "            ).long()\n",
    "\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.num_processes > 1:\n",
    "                grads = text_encoder.module.get_input_embeddings().weight.grad\n",
    "            else:\n",
    "                grads = text_encoder.get_input_embeddings().weight.grad\n",
    "\n",
    "            index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            if global_step % 500 == 0:\n",
    "                learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
    "                learned_embeds_dict = {placeholder: learned_embeds.detach().cpu()}\n",
    "                torch.save(learned_embeds_dict, os.path.join(\"./textual_inversion_outputs/\", \"learned_embeds.bin\"))\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if global_step >= 3000:\n",
    "            break\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "# Create the pipeline using using the trained modules and save it.\n",
    "if accelerator.is_main_process:\n",
    "    pipeline = StableDiffusionPipeline(\n",
    "        text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "        vae=vae,\n",
    "        unet=model,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear', num_train_timesteps=1000, set_alpha_to_one=False, skip_prk_steps=True, steps_offset=1, clip_sample=False),\n",
    "        safety_checker=None,\n",
    "        feature_extractor=CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "    )\n",
    "    pipeline.save_pretrained(\"./textual_inversion_outputs/\")\n",
    "    \n",
    "    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
    "    learned_embeds_dict = {placeholder: learned_embeds.detach().cpu()}\n",
    "    torch.save(learned_embeds_dict, os.path.join(\"./textual_inversion_outputs/\", \"learned_embeds.bin\"))\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"Compare this script to this:\n",
    "    !git clone https://github.com/justinpinkney/stable-diffusion.git\n",
    "    %cd stable-diffusion\n",
    "    !pip install --upgrade pip\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    !pip install --upgrade keras # on lambda stack we need to upgrade keras\n",
    "    !pip uninstall -y torchtext # on colab we need to remove torchtext\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    ds = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train\")\n",
    "    sample = ds[0]\n",
    "    display(sample[\"image\"].resize((256, 256)))\n",
    "    print(sample[\"text\"])\n",
    "    \n",
    "    BATCH_SIZE = 4\n",
    "    N_GPUS = 1\n",
    "    ACCUMULATE_BATCHES = 1\n",
    "\n",
    "    gpu_list = \",\".join((str(x) for x in range(N_GPUS))) + \",\"\n",
    "    print(f\"Using GPUs: {gpu_list}\")\n",
    "    \n",
    "    !(python main.py \\\n",
    "        -t \\\n",
    "        --base configs/stable-diffusion/pokemon.yaml \\\n",
    "        --gpus \"$gpu_list\" \\\n",
    "        --scale_lr False \\\n",
    "        --num_nodes 1 \\\n",
    "        --check_val_every_n_epoch 10 \\\n",
    "        --finetune_from \"$ckpt_path\" \\\n",
    "        data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "        lightning.trainer.accumulate_grad_batches=\"$ACCUMULATE_BATCHES\" \\\n",
    "        data.params.validation.params.n_gpus=\"$NUM_GPUS\" \\\n",
    "    )\n",
    "    !(python scripts/txt2img.py \\\n",
    "        --prompt 'robotic cat with wings' \\\n",
    "        --outdir 'outputs/generated_pokemon' \\\n",
    "        --H 512 --W 512 \\\n",
    "        --n_samples 4 \\\n",
    "        --config 'configs/stable-diffusion/pokemon.yaml' \\\n",
    "        --ckpt 'path/to/your/checkpoint')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in c:\\users\\chris\\appdata\\roaming\\python\\python310\\site-packages (22.3)\n",
      "Collecting pip\n",
      "  Using cached pip-22.3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3\n",
      "    Uninstalling pip-22.3:\n",
      "      Successfully uninstalled pip-22.3\n",
      "Successfully installed pip-22.3.1\n"
     ]
    }
   ],
   "source": [
    "#Dreambooth Locally Fine-Tune stable diffusion\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision=\"fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\": \"photo of a zwx pokemon\",\n",
    "        \"class_prompt\": \"photo of a pokemon\",\n",
    "        \"instance_data_dir\": \"training/pokemon\",\n",
    "        \"class_data_dir\": \"classes/pokemon\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_dir = Path(concepts_list[0][\"class_data_dir\"])\n",
    "curr_class_images = len(list(class_image_dir.iterdir()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Only run this if you have new images that you're adding to a dataset\n",
    "torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"./models/sd_v1-5/\",\n",
    "    vae=AutoencoderKL.from_pretrained(\n",
    "        \"./models/vae/\"\n",
    "    ),\n",
    "    torch_dtype=torch_dtype,\n",
    "    safety_checker=None\n",
    ")\n",
    "pipeline.to(accelerator.device)\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example\n",
    "    \n",
    "sample_dataset = PromptDataset(concepts_list[0][\"class_prompt\"], num_samples=0) #Tell it the number of new images\n",
    "sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=1)\n",
    "\n",
    "sample_dataloader = accelerator.prepare(sample_dataloader)\n",
    "\n",
    "\n",
    "with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "    for example in tqdm(\n",
    "        sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n",
    "    ):\n",
    "        images = pipeline(example[\"prompt\"]).images\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n",
    "            image_filename = class_image_dir / f\"{example['index'][i] + curr_class_images}-{hash_image}.jpg\"\n",
    "            image.save(image_filename)\n",
    "            \n",
    "del pipeline\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained('./models/vae/').to(\"cuda\")\n",
    "vae.requires_grad_(False)\n",
    "tokenizer = CLIPTokenizer.from_pretrained('./tokenizers/')\n",
    "text_encoder = CLIPTextModel.from_pretrained('./models/text_encoder/').to(\"cuda\")\n",
    "model = UNet2DConditionModel.from_pretrained('./models/sd_v1-5/').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use 8-bit Adam\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "except ImportError:\n",
    "    print(\"sorry dude\")\n",
    "    \n",
    "if not bnb:\n",
    "    optimizer_class = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_optimize = (itertools.chain(model.parameters(), text_encoder.parameters()))\n",
    "\n",
    "optimizer = optimizer_class(\n",
    "    params_to_optimize,\n",
    "    lr=1e-6, #If you're training your text encoder too, else 5e-6\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-08\n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    beta_start=0.00085, \n",
    "    beta_end=0.012, \n",
    "    beta_schedule='scaled_linear', \n",
    "    num_train_timesteps=1000, \n",
    "    set_alpha_to_one=False, \n",
    "    skip_prk_steps=True, \n",
    "    steps_offset=1, \n",
    "    clip_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        concepts_list,\n",
    "        tokenizer,\n",
    "        with_prior_preservation=True,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "        num_class_images=None,\n",
    "        pad_tokens=False,\n",
    "        hflip=False\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.with_prior_preservation = with_prior_preservation\n",
    "        self.pad_tokens = pad_tokens\n",
    "\n",
    "        self.instance_images_path = []\n",
    "        self.class_images_path = []\n",
    "\n",
    "        for concept in concepts_list:\n",
    "            inst_img_path = [(x, concept[\"instance_prompt\"]) for x in Path(concept[\"instance_data_dir\"]).iterdir() if x.is_file()]\n",
    "            self.instance_images_path.extend(inst_img_path)\n",
    "\n",
    "            if with_prior_preservation:\n",
    "                class_img_path = [(x, concept[\"class_prompt\"]) for x in Path(concept[\"class_data_dir\"]).iterdir() if x.is_file()]\n",
    "                self.class_images_path.extend(class_img_path[:num_class_images])\n",
    "\n",
    "        random.shuffle(self.instance_images_path)\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.num_class_images = len(self.class_images_path)\n",
    "        self._length = max(self.num_class_images, self.num_instance_images)\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(0.5 * hflip),\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_path, instance_prompt = self.instance_images_path[index % self.num_instance_images]\n",
    "        instance_image = Image.open(instance_path)\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            instance_prompt,\n",
    "            padding=\"max_length\" if self.pad_tokens else \"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        if self.with_prior_preservation:\n",
    "            class_path, class_prompt = self.class_images_path[index % self.num_class_images]\n",
    "            class_image = Image.open(class_path)\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                class_prompt,\n",
    "                padding=\"max_length\" if self.pad_tokens else \"do_not_pad\",\n",
    "                truncation=True,\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "            ).input_ids\n",
    "\n",
    "        return example\n",
    "    \n",
    "train_dataset = DreamBoothDataset(\n",
    "    concepts_list=concepts_list,\n",
    "    tokenizer=tokenizer,\n",
    "    with_prior_preservation=True,\n",
    "    size=512,\n",
    "    center_crop=False,\n",
    "    num_class_images=50,\n",
    "    pad_tokens=True,\n",
    "    hflip=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(examples):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    \n",
    "    input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "    pixel_values += [example[\"class_prompt_ids\"] for example in examples]\n",
    "    \n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    input_ids = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, collate_fn=collate, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_type = \"fp16\"\n",
    "vae.to(accelerator.device, dtype=weight_type)\n",
    "\n",
    "latents_cache = []\n",
    "text_encoder_cache = []\n",
    "for batch in tqdm(train_dataloader, desc=\"Caching latents\"):\n",
    "    with torch.no_grad():\n",
    "        batch['pixel_values'] = batch['pixel_values'].to(accelerator.device, non_blocking=True, dtype=weight_dtype)\n",
    "        batch['input_ids'] = batch['input_ids'].to(accelerator.device, non_blocking=True)\n",
    "        latents_cache.append(vae.encode(batch[\"pixel_values\"]).latent_dist)\n",
    "        \n",
    "        text_encoder_cache.append(batch['input_ids'])\n",
    "        \n",
    "class LatentsDataset(Dataset):\n",
    "    def __init__(self, latents_cache, text_encoder_cache):\n",
    "        self.latents_cache = latents_cache\n",
    "        self.text_encoder_cache = text_encoder_cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents_cache)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.latents_cache[index], self.text_encoder_cache[index]\n",
    "    \n",
    "train_dataset = LatentsDataset(latents_cache, text_encoder_cache)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, collate_fn=lambda x: x, shuffle=True\n",
    ")\n",
    "\n",
    "del vae\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / 1)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"constant\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = 800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, text_encoder, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / 1)\n",
    "num_train_epochs = math.ceil(800 / num_update_steps_per_epoch)\n",
    "\n",
    "total_batch_size = 1 * accelerator.num_processes * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(step):\n",
    "    if accelerator.is_main_process:\n",
    "        text_enc_model = accelerator.unwrap_model(text_encoder)\n",
    "        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            text_encoder=text_enc_model,\n",
    "            vae=AutoencoderKL.from_pretrained(\n",
    "                './models/vae/'\n",
    "            ),\n",
    "            safety_checker=None,\n",
    "            scheduler=scheduler,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        save_dir = \"./dreambooth_outputs/\"\n",
    "        pipeline.save_pretrained(save_dir)\n",
    "        \n",
    "        pipeline = pipeline.to(accelerator.device)\n",
    "        g_cuda = torch.Generator(device=accelerator.device).manual_seed(8855) #This is arbitrary, I just like this one.\n",
    "        pipeline.set_progress_bar_config(disable=True)\n",
    "        sample_dir = os.path.join(save_dir, \"samples\")\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        with torch.autocast(\"cuda\"), torch.inference_mode():\n",
    "            for i in tqdm(range(4)): #This number is how many samples of the new model to save\n",
    "                images = pipeline(\n",
    "                    \"photo of a zwx pokemon\",\n",
    "                    negative_prompt=\"\", #Add negative prompts\n",
    "                    guidance_scale=7.5, #default\n",
    "                    num_inference_steps=50, #default\n",
    "                    generator=g_cuda\n",
    "                ).images\n",
    "                images[0].save(os.path.join(sample_dir, f\"{i}.png\"))\n",
    "        del pipeline\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    print(f\"[*] Weights saved at {save_dir}\")\n",
    "    \n",
    "class AverageMeter:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = self.count = self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "progress_bar = tqdm(range(800), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Train_steps\")\n",
    "global_step = 0\n",
    "loss_avg = AverageMeter()\n",
    "text_enc_context = nullcontext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            with torch.no_grad():\n",
    "                latent_dist = batch[0][0]\n",
    "                latents = latent_dist.sample() * 0.18215\n",
    "                \n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            \n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "            \n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            with text_enc_context:\n",
    "                encoder_hidden_states = text_encoder(batch[0][1])[0]\n",
    "                \n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n",
    "            noise, noise_prior = torch.chunk(noise, 2, dim=0)\n",
    "            \n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"none\").mean([1,2,3]).mean()\n",
    "            \n",
    "            prior_loss = F.mse_loss(noise_pred_prior.float(), noise_prior.float(), reduction=\"mean\")\n",
    "            \n",
    "            loss = loss + 1.0*prior_loss\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss_avg.update(loss.detach(), bsz)\n",
    "            \n",
    "        if not global_step % 10:\n",
    "            logs = {\"loss\": loss_avg.avg.item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            \n",
    "        if global_step > 0 and not global_step % 800:\n",
    "            save_weights(global_step)\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step >= 800:\n",
    "            break\n",
    "        \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "save_weights(global_step)\n",
    "\n",
    "accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
