{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level 1\n",
    "# !pip install --upgrade diffusers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade tokenizers\n",
    "# !pip install --upgrade datasets\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_auth_token=access_token).to(\"cuda\") #use revision='fp16' and torch_dtype=torch.float16 for low memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of a horse riding an astronaut on Mars\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"./images/horse_rides_astronaut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 2\n",
    "from torch import autocast\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(images, **kwargs):\n",
    "    return images, False\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.safety_checker = dummy\n",
    "n_images = 3\n",
    "prompts = [\n",
    "    \"masterpiece, best quality, a photo of a horse riding an astronaut, trending on artstation, photorealistic, qhd, rtx on, 8k\"\n",
    "] * n_images\n",
    "with autocast(\"cuda\"):\n",
    "    images = pipe(prompts, num_inference_steps=28).images\n",
    "image_grid(images, rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#level 3\n",
    "from diffusers import UNet2DConditionModel, StableDiffusionPipeline, AutoencoderKL, LMSDiscreteScheduler, DDIMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import autocast\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder='vae', use_auth_token=access_token)\n",
    "# vae.save_pretrained('./models/vae.ckpt')\n",
    "vae = AutoencoderKL.from_pretrained('./models/vae/').to(\"cuda\")\n",
    "\n",
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# tokenizer.save_pretrained('./tokenizers/')\n",
    "tokenizer = CLIPTokenizer.from_pretrained('./tokenizers/')\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n",
    "# text_encoder.save_pretrained('./models/text_encoder.pt')\n",
    "text_encoder = CLIPTextModel.from_pretrained('./models/text_encoder/').to(\"cuda\")\n",
    "\n",
    "# model = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder='unet', use_auth_token=access_token).to(\"cuda\")\n",
    "# model.save_pretrained('./models/sd_v1-5.ckpt')\n",
    "model = UNet2DConditionModel.from_pretrained('./models/sd_v1-5.ckpt/').to(\"cuda\")\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear', num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeds(prompt):\n",
    "  # Tokenize text and get embeddings\n",
    "  text_input = tokenizer(\n",
    "      prompt, padding='max_length', max_length=tokenizer.model_max_length,\n",
    "      truncation=True, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n",
    "\n",
    "  # Do the same for unconditional embeddings\n",
    "  uncond_input = tokenizer(\n",
    "      [''] * len(prompt), padding='max_length',\n",
    "      max_length=tokenizer.model_max_length, return_tensors='pt')\n",
    "  with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n",
    "\n",
    "  # Cat for final embeddings\n",
    "  text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "  return text_embeddings\n",
    "\n",
    "test_embeds = get_text_embeds(['an amazingly cool anime character'])\n",
    "print(test_embeds)\n",
    "print(test_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "                    num_inference_steps=28, guidance_scale=11, latents=None,\n",
    "                    return_all_latents=False):\n",
    "  if latents is None:\n",
    "    latents = torch.randn((text_embeddings.shape[0] // 2, model.in_channels, \\\n",
    "                           height // 8, width // 8))\n",
    "  latents = latents.to(\"cuda\")\n",
    "\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "  latents = latents * scheduler.sigmas[0]\n",
    "\n",
    "  latent_hist = [latents]\n",
    "  with autocast('cuda'):\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "      sigma = scheduler.sigmas[i]\n",
    "      latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = model(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, i, latents)['prev_sample']\n",
    "      latent_hist.append(latents)\n",
    "  \n",
    "  if not return_all_latents:\n",
    "    return latents\n",
    "\n",
    "  all_latents = torch.cat(latent_hist, dim=0)\n",
    "  return all_latents\n",
    "\n",
    "test_latents = produce_latents(test_embeds)\n",
    "print(test_latents)\n",
    "print(test_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img_latents(latents):\n",
    "  latents = 1 / 0.18215 * latents\n",
    "\n",
    "  with torch.no_grad():\n",
    "    imgs = vae.decode(latents)['sample']\n",
    "\n",
    "  imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "  imgs = imgs.detach().cpu().permute(0, 2, 3, 1)\n",
    "  imgs = (imgs + 1.0) * 127.5\n",
    "  imgs = imgs.numpy().astype(np.uint8)\n",
    "  pil_images = [Image.fromarray(image) for image in imgs]\n",
    "  return pil_images\n",
    "\n",
    "imgs = decode_img_latents(test_latents)\n",
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=28, guidance_scale=11, latents=None):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "        \n",
    "    #Prompts -> text embeddings\n",
    "    text_embeds = get_text_embeds(prompts)\n",
    "    \n",
    "    #Text embeddings -> img latents\n",
    "    latents = produce_latents(text_embeds, height=height, width=width, latents=latents, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale)\n",
    "    \n",
    "    #Img latents -> imgs\n",
    "    imgs = decode_img_latents(latents)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = prompt_to_img(['Super cool fantasty knight, intricate armor, 8k']*4, 512, 512, 28, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_grid(imgs, rows=2, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level 3.5 - similar images and img2img\n",
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
    "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
    "                  batch_size=2):\n",
    "  if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "\n",
    "  # Prompts -> text embeds\n",
    "  text_embeds = get_text_embeds(prompts)\n",
    "\n",
    "  # Text embeds -> img latents\n",
    "  latents = produce_latents(\n",
    "      text_embeds, height=height, width=width, latents=latents,\n",
    "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "      return_all_latents=return_all_latents)\n",
    "  \n",
    "  # Img latents -> imgs\n",
    "  all_imgs = []\n",
    "  for i in tqdm(range(0, len(latents), batch_size)):\n",
    "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
    "    all_imgs.extend(imgs)\n",
    "\n",
    "  return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Steampunk airship bursting through the clouds, cyberpunk art'\n",
    "latents = torch.randn((1, model.in_channels, 512 // 8, 512 // 8))\n",
    "img = prompt_to_img(prompt, num_inference_steps=20, latents=latents)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_latents(latents, scale=0.1):\n",
    "  noise = torch.randn_like(latents)\n",
    "  new_latents = (1 - scale) * latents + scale * noise\n",
    "  return (new_latents - new_latents.mean()) / new_latents.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_latents = perturb_latents(latents, 0.4)\n",
    "img = prompt_to_img(prompt, num_inference_steps=20, latents=new_latents)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Upright squid'\n",
    "img = prompt_to_img(prompt, num_inference_steps=30)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img_latents(imgs):\n",
    "  if not isinstance(imgs, list):\n",
    "    imgs = [imgs]\n",
    "\n",
    "  img_arr = np.stack([np.array(img) for img in imgs], axis=0)\n",
    "  img_arr = img_arr / 255.0\n",
    "  img_arr = torch.from_numpy(img_arr).float().permute(0, 3, 1, 2)\n",
    "  img_arr = 2 * (img_arr - 0.5)\n",
    "\n",
    "  latent_dists = vae.encode(img_arr.to(\"cuda\"))\n",
    "  latent_samples = latent_dists.latent_dist.sample()\n",
    "  latent_samples *= 0.18215\n",
    "\n",
    "  return latent_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_latents = encode_img_latents([img])\n",
    "dec_img = decode_img_latents(img_latents)[0]\n",
    "dec_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New scheduler for img-to-img\n",
    "scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012,\n",
    "    beta_schedule='scaled_linear', num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_latents(text_embeddings, height=512, width=512,\n",
    "                    num_inference_steps=50, guidance_scale=7.5, latents=None,\n",
    "                    return_all_latents=False, start_step=10):\n",
    "  if latents is None:\n",
    "    latents = torch.randn((text_embeddings.shape[0] // 2, model.in_channels, \\\n",
    "                           height // 8, width // 8))\n",
    "  latents = latents.to(\"cuda\")\n",
    "\n",
    "  scheduler.set_timesteps(num_inference_steps)\n",
    "  if start_step > 0:\n",
    "    start_timestep = scheduler.timesteps[start_step]\n",
    "    start_timesteps = start_timestep.repeat(latents.shape[0]).long()\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    latents = scheduler.add_noise(latents, noise, start_timesteps)\n",
    "\n",
    "  latent_hist = [latents]\n",
    "  with autocast('cuda'):\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps[start_step:])):\n",
    "      # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "      # predict the noise residual\n",
    "      with torch.no_grad():\n",
    "        noise_pred = model(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "      # perform guidance\n",
    "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "      noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "      # compute the previous noisy sample x_t -> x_t-1\n",
    "      latents = scheduler.step(noise_pred, t, latents)['prev_sample']\n",
    "      latent_hist.append(latents)\n",
    "  \n",
    "  if not return_all_latents:\n",
    "    return latents\n",
    "\n",
    "  all_latents = torch.cat(latent_hist, dim=0)\n",
    "  return all_latents\n",
    "\n",
    "def prompt_to_img(prompts, height=512, width=512, num_inference_steps=50,\n",
    "                  guidance_scale=7.5, latents=None, return_all_latents=False,\n",
    "                  batch_size=2, start_step=0):\n",
    "  if isinstance(prompts, str):\n",
    "    prompts = [prompts]\n",
    "\n",
    "  # Prompts -> text embeds\n",
    "  text_embeds = get_text_embeds(prompts)\n",
    "\n",
    "  # Text embeds -> img latents\n",
    "  latents = produce_latents(\n",
    "      text_embeds, height=height, width=width, latents=latents,\n",
    "      num_inference_steps=num_inference_steps, guidance_scale=guidance_scale,\n",
    "      return_all_latents=return_all_latents, start_step=start_step)\n",
    "  \n",
    "  # Img latents -> imgs\n",
    "  all_imgs = []\n",
    "  for i in tqdm(range(0, len(latents), batch_size)):\n",
    "    imgs = decode_img_latents(latents[i:i+batch_size])\n",
    "    all_imgs.extend(imgs)\n",
    "\n",
    "  return all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Squidward'\n",
    "img = prompt_to_img(prompt, num_inference_steps=30, latents=img_latents,\n",
    "                    start_step=20)[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 4 - AUTOMATIC1111\n",
    "#!git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
    "#https://rentry.org/voldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Level 5 - Deforum, Vid2Vid, Textual Inversion (Dreambooth), Negative Prompts, Fine-Tuning, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
